{"version":3,"sources":["App.js","index.js"],"names":["blazeface","require","classes","App","webcamRef","useRef","canvasRef","runCoco","a","tf","process","net","load","modelFaces","setInterval","detectFaces","current","video","readyState","videoWidth","videoHeight","width","height","ctx","getContext","img","fromPixels","estimateFaces","predictions","length","start","topLeft","end","bottomRight","size","console","log","crop_height","crop_width","shape","slicedImg","slice","Math","round","resized","resizeBilinear","div","cast","expanded","expandDims","pred","predict","dataSync","fc","fp","nf","color","text","value","requestAnimationFrame","strokeStyle","lineWidth","fillStyle","font","beginPath","fillText","rect","stroke","useEffect","className","ref","muted","style","position","marginLeft","marginRight","left","right","textAlign","zindex","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"4YAOMA,EAAYC,EAAQ,KAEpBC,EAAU,CAAC,mBAAoB,gBAAiB,gBA0LvCC,MAxLf,WACE,IAAMC,EAAYC,iBAAO,MACnBC,EAAYD,iBAAO,MAGnBE,EAAO,uCAAG,8BAAAC,EAAA,sEAIIC,IAChBC,kGALY,cAIRC,EAJQ,gBAOWX,EAAUY,OAPrB,OAORC,EAPQ,OAUdC,aAAY,WACVC,EAAYF,EAAYF,KACvB,MAZW,2CAAH,qDAePI,EAAW,uCAAG,WAAOF,EAAYF,GAAnB,yDAAAH,EAAA,yDAGa,qBAAtBJ,EAAUY,SACK,OAAtBZ,EAAUY,SAC6B,IAAvCZ,EAAUY,QAAQC,MAAMC,WALR,wBAQVD,EAAQb,EAAUY,QAAQC,MAC1BE,EAAaf,EAAUY,QAAQC,MAAME,WACrCC,EAAchB,EAAUY,QAAQC,MAAMG,YAG5ChB,EAAUY,QAAQC,MAAMI,MAAQF,EAChCf,EAAUY,QAAQC,MAAMK,OAASF,EAGjCd,EAAUU,QAAQK,MAAQF,EAC1Bb,EAAUU,QAAQM,OAASF,EAGrBG,EAAMjB,EAAUU,QAAQQ,WAAW,MACnCC,EAAMhB,IAAWiB,WAAWT,IAKZ,EA3BN,UA4BUJ,EAAWc,cAAcF,EAD7B,OA3BN,SA4BVG,EA5BU,QA8BAC,OAAS,GAAKD,EAAYC,OAAS,IAC3CC,EAAQF,EAAY,GAAGG,QACvBC,EAAMJ,EAAY,GAAGK,YAErBC,EAAO,CAACF,EAAI,GAAKF,EAAM,GAAIE,EAAI,GAAKF,EAAM,IAEhDK,QAAQC,IAAI,kBAMRC,EAAcH,EAAK,IADnBI,EAAaJ,EAAK,IAILT,EAAIc,MAAM,GAAKT,EAAM,KAClCQ,EAAcb,EAAIc,MAAM,GAAKT,EAAM,IAInCO,EAAcZ,EAAIc,MAAM,GAAKT,EAAM,KACnCO,EAAeZ,EAAIc,MAAM,GAAKT,EAAM,IAExCK,QAAQC,IAAIX,EAAIc,OAChBJ,QAAQC,IAAIF,GACZC,QAAQC,IAAIN,GAERU,EAAYf,EAAIgB,MAAM,CAACX,EAAM,GAAG,GAAIA,EAAM,IAAK,CAACY,KAAKC,MAAMN,GAAa,GAAGK,KAAKC,MAAML,KACpFM,EAAUnC,IACboC,eAAeL,EAAW,CAAC,IAAK,MAChCM,IAAIrC,IAAU,MACXsC,EAAOtC,IAAQmC,EAAS,WACxBI,EAAWD,EAAKE,WAAW,GAC3BC,EAAOvC,EAAIwC,QAAQH,GAAUI,WAE/BC,EAAKH,EAAK,GACVI,EAAKJ,EAAK,GACVK,EAAKL,EAAK,GAEVM,EAAQ,MACRC,EAAO,GACPC,EAAQ,EACRL,EAAKC,GAAMD,EAAKE,GAClBG,EAAQL,EACRG,EAAQ,OACRC,EAAOvD,EAAQ,IACNoD,EAAKD,GAAMC,EAAKC,GACzBG,EAAQJ,EACRE,EAAQ,SACRC,EAAOvD,EAAQ,KAEfwD,EAAQH,EACRC,EAAQ,MACRC,EAAOvD,EAAQ,IAKjByD,uBAAsB,WAGpBpC,EAAIqC,YAAcJ,EAClBjC,EAAIsC,UAAY,GAChBtC,EAAIuC,UAAY,QAChBvC,EAAIwC,KAAO,aAKXxC,EAAIyC,YACJzC,EAAI0C,SACFR,EAAO,MAAQf,KAAKC,MAAc,IAARe,GAAe,IACzC5B,EAAM,GAAI,GAAKA,EAAM,GAAG,IAE1BP,EAAI2C,KAAKpC,EAAM,GAAIA,EAAM,GAAG,GAAII,EAAK,GAAIA,EAAK,GAAG,IAIjDX,EAAI4C,YAIN1D,IAAWgB,GACXhB,IAAWmC,GACXnC,IAAWsC,GACXtC,IAAWuC,IAGbvC,IAAWgB,GAtHK,4CAAH,wDA8HjB,OAJA2C,qBAAU,WACR7D,MACC,IAGD,qBAAK8D,UAAU,MAAf,SACE,yBAAQA,UAAU,aAAlB,UACE,cAAC,IAAD,CACEC,IAAKlE,EACLmE,OAAO,EACPC,MAAO,CACLC,SAAU,WACVC,WAAY,OACZC,YAAa,OACbC,KAAM,EACNC,MAAO,EACPC,UAAW,SACXC,OAAQ,EACR1D,MAAO,IACPC,OAAQ,OAIZ,wBACEgD,IAAKhE,EACLkE,MAAO,CACLC,SAAU,WACVC,WAAY,OACZC,YAAa,OACbC,KAAM,EACNC,MAAO,EACPC,UAAW,SACXC,OAAQ,EACR1D,MAAO,IACPC,OAAQ,aCtLpB0D,IAASC,OACP,eAAC,IAAMC,WAAP,WACE,qBAAKb,UAAU,SAAf,SACE,oBAAIA,UAAU,cAAd,mCAEF,cAAC,EAAD,OAEFc,SAASC,eAAe,W","file":"static/js/main.b2a8eb45.chunk.js","sourcesContent":["// Import dependencies\r\nimport React, { useRef, useEffect } from \"react\";\r\nimport * as tf from \"@tensorflow/tfjs\";\r\nimport Webcam from \"react-webcam\";\r\n\r\nimport \"./App.css\";\r\n\r\nconst blazeface = require(\"@tensorflow-models/blazeface\");\r\n\r\nconst classes = [\"Correctly placed\", \"Poorly placed\", \"No face mask\"];\r\n\r\nfunction App() {\r\n  const webcamRef = useRef(null);\r\n  const canvasRef = useRef(null);\r\n\r\n  // Main function\r\n  const runCoco = async () => {\r\n    // 3. TODO - Load network\r\n    // e.g. const net = await cocossd.load();\r\n\r\n    const net = await tf.loadLayersModel(\r\n      process.env.REACT_APP_MODEL_URL\r\n    );\r\n    const modelFaces = await blazeface.load();\r\n\r\n    //  Loop and detect hands\r\n    setInterval(() => {\r\n      detectFaces(modelFaces, net);\r\n    }, 16.7);\r\n  };\r\n\r\n  const detectFaces = async (modelFaces, net) => {\r\n    // Check data is available\r\n    if (\r\n      typeof webcamRef.current !== \"undefined\" &&\r\n      webcamRef.current !== null &&\r\n      webcamRef.current.video.readyState === 4\r\n    ) {\r\n      // Get Video Properties\r\n      const video = webcamRef.current.video;\r\n      const videoWidth = webcamRef.current.video.videoWidth;\r\n      const videoHeight = webcamRef.current.video.videoHeight;\r\n\r\n      // Set video width\r\n      webcamRef.current.video.width = videoWidth;\r\n      webcamRef.current.video.height = videoHeight;\r\n\r\n      // Set canvas height and width\r\n      canvasRef.current.width = videoWidth;\r\n      canvasRef.current.height = videoHeight;\r\n\r\n      // Draw mesh\r\n      const ctx = canvasRef.current.getContext(\"2d\");\r\n      const img = tf.browser.fromPixels(video);\r\n\r\n      // Pass in an image or video to the model. The model returns an array of\r\n      // bounding boxes, probabilities, and landmarks, one for each detected face.\r\n\r\n      const returnTensors = false; // Pass in `true` to get tensors back, rather than values.\r\n      const predictions = await modelFaces.estimateFaces(img, returnTensors);\r\n\r\n      if (predictions.length > 0 && predictions.length < 2) {\r\n        const start = predictions[0].topLeft;\r\n        const end = predictions[0].bottomRight;\r\n        \r\n        const size = [end[0] - start[0], end[1] - start[1]];\r\n\r\n        console.log(\"One face found\");\r\n\r\n        // ------------------ Preprocess image --------------\r\n        //const slicedImg = tf.browser.fromPixels(ctx.getImageData(start[0]-10, start[1]-10, size[0]+10, size[1]+10));\r\n\r\n        let crop_width = size[0];\r\n        let crop_height = size[1];\r\n\r\n        // Get valid bbox width in canvas\r\n        if (crop_width > img.shape[1] - start[0]){\r\n            crop_width =  img.shape[1] - start[0];\r\n        }\r\n\r\n        // Get valid bbox height in canvas\r\n        if (crop_height > img.shape[0] - start[1]){\r\n            crop_height =  img.shape[0] - start[1];\r\n        }\r\n        console.log(img.shape)\r\n        console.log(size)\r\n        console.log(start)\r\n\r\n        var slicedImg = img.slice([start[1]-30, start[0]], [Math.round(crop_height)+30,Math.round(crop_width)]);\r\n        const resized = tf.image\r\n          .resizeBilinear(slicedImg, [224, 224])\r\n          .div(tf.scalar(255));\r\n        const cast = tf.cast(resized, \"float32\");\r\n        const expanded = cast.expandDims(0);\r\n        const pred = net.predict(expanded).dataSync();\r\n\r\n        let fc = pred[0];\r\n        let fp = pred[1];\r\n        let nf = pred[2];\r\n\r\n        let color = \"red\";\r\n        let text = \"\";\r\n        let value = 1.0;\r\n        if (fc > fp && fc > nf) {\r\n          value = fc;\r\n          color = \"blue\";\r\n          text = classes[0];\r\n        } else if (fp > fc && fp > nf) {\r\n          value = fp;\r\n          color = \"yellow\";\r\n          text = classes[1];\r\n        } else {\r\n          value = nf;\r\n          color = \"red\";\r\n          text = classes[2];\r\n        }\r\n\r\n       \r\n\r\n        requestAnimationFrame(() => {\r\n          // Render a rectangle over each detected face.\r\n          // Set styling\r\n          ctx.strokeStyle = color;\r\n          ctx.lineWidth = 10;\r\n          ctx.fillStyle = \"white\";\r\n          ctx.font = \"40px Arial\";\r\n          //\r\n   \r\n          //\r\n  \r\n          ctx.beginPath();\r\n          ctx.fillText(\r\n            text + \" - \" + Math.round(value * 100) / 100,\r\n            start[0] -50 , start[1]-50\r\n          );\r\n          ctx.rect(start[0], start[1]-30, size[0], size[1]+30);\r\n    \r\n        \r\n\r\n          ctx.stroke();\r\n        });\r\n        // Display the winner\r\n\r\n        tf.dispose(img);\r\n        tf.dispose(resized);\r\n        tf.dispose(cast);\r\n        tf.dispose(expanded);\r\n      }\r\n\r\n      tf.dispose(img);\r\n    }\r\n  };\r\n\r\n  useEffect(() => {\r\n    runCoco();\r\n  }, []);\r\n\r\n  return (\r\n    <div className=\"App\">\r\n      <header className=\"App-header\">\r\n        <Webcam\r\n          ref={webcamRef}\r\n          muted={true}\r\n          style={{\r\n            position: \"absolute\",\r\n            marginLeft: \"auto\",\r\n            marginRight: \"auto\",\r\n            left: 0,\r\n            right: 0,\r\n            textAlign: \"center\",\r\n            zindex: 9,\r\n            width: 224,\r\n            height: 224,\r\n          }}\r\n        />\r\n\r\n        <canvas\r\n          ref={canvasRef}\r\n          style={{\r\n            position: \"absolute\",\r\n            marginLeft: \"auto\",\r\n            marginRight: \"auto\",\r\n            left: 0,\r\n            right: 0,\r\n            textAlign: \"center\",\r\n            zindex: 8,\r\n            width: 224,\r\n            height: 224,\r\n          }}\r\n        />\r\n      </header>\r\n    </div>\r\n  );\r\n}\r\n\r\nexport default App;\r\n","import React from 'react';\r\nimport ReactDOM from 'react-dom';\r\nimport './index.css';\r\nimport App from './App';\r\n\r\nReactDOM.render(\r\n  <React.StrictMode>\r\n    <div className=\"header\">\r\n      <h1 className=\"text-center\">Face mask detection</h1>\r\n    </div>\r\n    <App />\r\n  </React.StrictMode>,\r\n  document.getElementById('root')\r\n);"],"sourceRoot":""}