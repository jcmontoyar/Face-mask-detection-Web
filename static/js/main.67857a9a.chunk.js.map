{"version":3,"sources":["App.js","index.js"],"names":["blazeface","require","classes","App","useState","messgLoading","setMessg","webcamRef","useRef","canvasRef","runCoco","a","tf","process","net","load","modelFaces","setInterval","detectFacesAndPredict","current","video","readyState","videoWidth","videoHeight","width","height","ctx","getContext","img","fromPixels","estimateFaces","predictions","length","start","topLeft","end","bottomRight","size","crop_height","crop_width","shape","slicedImg","slice","Math","round","resized","resizeBilinear","div","cast","expanded","expandDims","pred","predict","dataSync","fc","fp","nf","color","text","value","requestAnimationFrame","strokeStyle","lineWidth","fillStyle","font","beginPath","fillText","rect","stroke","useEffect","className","ref","muted","style","position","marginLeft","marginRight","left","right","textAlign","zindex","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"mZAOMA,EAAYC,EAAQ,KAEpBC,EAAU,CAAC,mBAAoB,gBAAiB,gBAiMvCC,MA/Lf,WACE,MAAiCC,mBAAS,kBAA1C,mBAAOC,EAAP,KAAqBC,EAArB,KACMC,EAAYC,iBAAO,MACnBC,EAAYD,iBAAO,MAGnBE,EAAO,uCAAG,8BAAAC,EAAA,sEAIIC,IAChBC,kGALY,cAIRC,EAJQ,gBAOWd,EAAUe,OAPrB,OAORC,EAPQ,OAQdV,EAAS,IAETW,aAAY,WACVC,EAAsBF,EAAYF,KACjC,MAZW,2CAAH,qDAePI,EAAqB,uCAAG,WAAOF,EAAYF,GAAnB,yDAAAH,EAAA,yDAGG,qBAAtBJ,EAAUY,SACK,OAAtBZ,EAAUY,SAC6B,IAAvCZ,EAAUY,QAAQC,MAAMC,WALE,wBAQpBD,EAAQb,EAAUY,QAAQC,MAC1BE,EAAaf,EAAUY,QAAQC,MAAME,WACrCC,EAAchB,EAAUY,QAAQC,MAAMG,YAG5ChB,EAAUY,QAAQC,MAAMI,MAAQF,EAChCf,EAAUY,QAAQC,MAAMK,OAASF,EAGjCd,EAAUU,QAAQK,MAAQF,EAC1Bb,EAAUU,QAAQM,OAASF,EAGrBG,EAAMjB,EAAUU,QAAQQ,WAAW,MAGnCC,EAAMhB,IAAWiB,WAAWT,IAGZ,EA3BI,UA4BAJ,EAAWc,cAAcF,EAD7B,OA3BI,SA4BpBG,EA5BoB,QA+BVC,OAAS,GAAKD,EAAYC,OAAS,IAG3CC,EAAQF,EAAY,GAAGG,QACvBC,EAAMJ,EAAY,GAAGK,YACrBC,EAAO,CAACF,EAAI,GAAKF,EAAM,GAAIE,EAAI,GAAKF,EAAM,IAI5CK,EAAcD,EAAK,IADnBE,EAAaF,EAAK,IAILT,EAAIY,MAAM,GAAKP,EAAM,KAClCM,EAAcX,EAAIY,MAAM,GAAKP,EAAM,IAInCK,EAAcV,EAAIY,MAAM,GAAKP,EAAM,KACnCK,EAAeV,EAAIY,MAAM,GAAKP,EAAM,IAKlCQ,EAAYb,EAAIc,MAAM,CAACT,EAAM,GAAG,GAAIA,EAAM,IAAK,CAACU,KAAKC,MAAMN,GAAa,GAAGK,KAAKC,MAAML,KAGtFM,EAAUjC,IACbkC,eAAeL,EAAW,CAAC,IAAK,MAChCM,IAAInC,IAAU,MAGXoC,EAAOpC,IAAQiC,EAAS,WAGxBI,EAAWD,EAAKE,WAAW,GAG3BC,EAAOrC,EAAIsC,QAAQH,GAAUI,WAG/BC,EAAKH,EAAK,GACVI,EAAKJ,EAAK,GACVK,EAAKL,EAAK,GAGVM,EAAQ,MACRC,EAAO,GACPC,EAAQ,EACRL,EAAKC,GAAMD,EAAKE,GAClBG,EAAQL,EACRG,EAAQ,OACRC,EAAOxD,EAAQ,IACNqD,EAAKD,GAAMC,EAAKC,GACzBG,EAAQJ,EACRE,EAAQ,SACRC,EAAOxD,EAAQ,KAEfyD,EAAQH,EACRC,EAAQ,MACRC,EAAOxD,EAAQ,IAKjB0D,uBAAsB,WAGpBlC,EAAImC,YAAcJ,EAClB/B,EAAIoC,UAAY,GAChBpC,EAAIqC,UAAY,QAChBrC,EAAIsC,KAAO,aAGXtC,EAAIuC,YACJvC,EAAIwC,SACFR,EAAO,MAAQf,KAAKC,MAAc,IAARe,GAAe,IACzC1B,EAAM,GAAI,GAAKA,EAAM,GAAG,IAE1BP,EAAIyC,KAAKlC,EAAM,GAAIA,EAAM,GAAG,GAAII,EAAK,GAAIA,EAAK,GAAG,IAIjDX,EAAI0C,YAINxD,IAAWgB,GACXhB,IAAWiC,GACXjC,IAAWoC,GACXpC,IAAWqC,GACXrC,IAAW6B,IAGb7B,IAAWgB,GA5He,4CAAH,wDAoI3B,OAJAyC,qBAAU,WACR3D,MACC,IAGD,sBAAK4D,UAAU,MAAf,UAEI,cAAC,IAAD,CACEC,IAAKhE,EACLiE,OAAO,EACPC,MAAO,CACLC,SAAU,WACVC,WAAY,OACZC,YAAa,OACbC,KAAM,EACNC,MAAO,EACPC,UAAW,SACXC,OAAQ,EACRxD,MAAO,IACPC,OAAQ,OAIZ,wBACE8C,IAAK9D,EACLgE,MAAO,CACLC,SAAU,WACVC,WAAY,OACZC,YAAa,OACbC,KAAM,EACNC,MAAO,EACPC,UAAW,SACXC,OAAQ,EACRxD,MAAO,IACPC,OAAQ,OAGd,mBAAG6C,UAAU,cAAb,SAA4BjE,QChMlC4E,IAASC,OACP,eAAC,IAAMC,WAAP,WACE,qBAAKb,UAAU,SAAf,SACE,oBAAIA,UAAU,cAAd,mCAEF,cAAC,EAAD,OAEFc,SAASC,eAAe,W","file":"static/js/main.67857a9a.chunk.js","sourcesContent":["// Import dependencies\r\nimport React, { useRef, useEffect, useState } from \"react\";\r\nimport * as tf from \"@tensorflow/tfjs\";\r\nimport Webcam from \"react-webcam\";\r\n\r\nimport \"./App.css\";\r\n\r\nconst blazeface = require(\"@tensorflow-models/blazeface\");\r\n\r\nconst classes = [\"Correctly placed\", \"Poorly placed\", \"No face mask\"];\r\n\r\nfunction App() {\r\n  const [messgLoading, setMessg] = useState(\"Loading models\");\r\n  const webcamRef = useRef(null);\r\n  const canvasRef = useRef(null);\r\n\r\n  // Main function\r\n  const runCoco = async () => {\r\n    // 3. TODO - Load network\r\n    // e.g. const net = await cocossd.load();\r\n\r\n    const net = await tf.loadLayersModel(\r\n      process.env.REACT_APP_MODEL_URL\r\n    );\r\n    const modelFaces = await blazeface.load();\r\n    setMessg(\"\");  \r\n    //  Loop and detect hands\r\n    setInterval(() => {\r\n      detectFacesAndPredict(modelFaces, net);\r\n    }, 16.7);\r\n  };\r\n\r\n  const detectFacesAndPredict = async (modelFaces, net) => {\r\n    // Check data is available\r\n    if (\r\n      typeof webcamRef.current !== \"undefined\" &&\r\n      webcamRef.current !== null &&\r\n      webcamRef.current.video.readyState === 4\r\n    ) {\r\n      // Get Video Properties\r\n      const video = webcamRef.current.video;\r\n      const videoWidth = webcamRef.current.video.videoWidth;\r\n      const videoHeight = webcamRef.current.video.videoHeight;\r\n\r\n      // Set video width\r\n      webcamRef.current.video.width = videoWidth;\r\n      webcamRef.current.video.height = videoHeight;\r\n\r\n      // Set canvas height and width\r\n      canvasRef.current.width = videoWidth;\r\n      canvasRef.current.height = videoHeight;\r\n\r\n      // Draw mesh\r\n      const ctx = canvasRef.current.getContext(\"2d\");\r\n\r\n      // Get tensor with tf.fromPixels and passing the current frame\r\n      const img = tf.browser.fromPixels(video);\r\n\r\n      // bounding boxes, probabilities, and landmarks, one for each detected face.\r\n      const returnTensors = false; // Pass in `true` to get tensors back, rather than values.\r\n      const predictions = await modelFaces.estimateFaces(img, returnTensors);\r\n\r\n      // We will predict only when one face found\r\n      if (predictions.length > 0 && predictions.length < 2) {\r\n\r\n        // Bounding box properties\r\n        const start = predictions[0].topLeft;\r\n        const end = predictions[0].bottomRight;\r\n        const size = [end[0] - start[0], end[1] - start[1]];\r\n\r\n        // ------------------ Preprocess image --------------\r\n        let crop_width = size[0];\r\n        let crop_height = size[1];\r\n\r\n        // Get valid bbox width in canvas\r\n        if (crop_width > img.shape[1] - start[0]){\r\n            crop_width =  img.shape[1] - start[0];\r\n        }\r\n\r\n        // Get valid bbox height in canvas\r\n        if (crop_height > img.shape[0] - start[1]){\r\n            crop_height =  img.shape[0] - start[1];\r\n        }\r\n\r\n        // Slice the image using tf.slice. We make the bounding box a bit bigger since the face masks usually make the face look bigger.\r\n        // Also de bounding box of the blazeface model skips the top of the head and the model trained had this part of the head in most images\r\n        const slicedImg = img.slice([start[1]-30, start[0]], [Math.round(crop_height)+30,Math.round(crop_width)]);\r\n\r\n        // Resize and normalize  to the same training dimensions used\r\n        const resized = tf.image\r\n          .resizeBilinear(slicedImg, [224, 224])\r\n          .div(tf.scalar(255));\r\n        \r\n        // Cast to the same data type you preprocessed your images when training the model\r\n        const cast = tf.cast(resized, \"float32\");\r\n\r\n        // Expand dimensions to match the following shape (224, 224, 3) this is the shape of the images used in training\r\n        const expanded = cast.expandDims(0);\r\n\r\n        // Get the prediction\r\n        const pred = net.predict(expanded).dataSync();\r\n\r\n        // Get the predicted label (higher confidence)\r\n        let fc = pred[0];\r\n        let fp = pred[1];\r\n        let nf = pred[2];\r\n\r\n        // Get colors and predicted label\r\n        let color = \"red\";\r\n        let text = \"\";\r\n        let value = 1.0;\r\n        if (fc > fp && fc > nf) {\r\n          value = fc;\r\n          color = \"blue\";\r\n          text = classes[0];\r\n        } else if (fp > fc && fp > nf) {\r\n          value = fp;\r\n          color = \"yellow\";\r\n          text = classes[1];\r\n        } else {\r\n          value = nf;\r\n          color = \"red\";\r\n          text = classes[2];\r\n        }\r\n\r\n       \r\n        // Draw the bounding box given the predicted label\r\n        requestAnimationFrame(() => {\r\n\r\n          // Set styling\r\n          ctx.strokeStyle = color;\r\n          ctx.lineWidth = 10;\r\n          ctx.fillStyle = \"white\";\r\n          ctx.font = \"40px Arial\";\r\n     \r\n          // Create rectangle\r\n          ctx.beginPath();\r\n          ctx.fillText(\r\n            text + \" - \" + Math.round(value * 100) / 100,\r\n            start[0] -50 , start[1]-50\r\n          );\r\n          ctx.rect(start[0], start[1]-30, size[0], size[1]+30);\r\n    \r\n        \r\n\r\n          ctx.stroke();\r\n        });\r\n        // Display the winner\r\n\r\n        tf.dispose(img);\r\n        tf.dispose(resized);\r\n        tf.dispose(cast);\r\n        tf.dispose(expanded);\r\n        tf.dispose(slicedImg);\r\n      }\r\n\r\n      tf.dispose(img);\r\n    }\r\n  };\r\n\r\n  useEffect(() => {\r\n    runCoco();\r\n  }, []);\r\n\r\n  return (\r\n    <div className=\"App\">\r\n  \r\n        <Webcam\r\n          ref={webcamRef}\r\n          muted={true}\r\n          style={{\r\n            position: \"absolute\",\r\n            marginLeft: \"auto\",\r\n            marginRight: \"auto\",\r\n            left: 0,\r\n            right: 0,\r\n            textAlign: \"center\",\r\n            zindex: 9,\r\n            width: 224,\r\n            height: 224,\r\n          }}\r\n        />\r\n\r\n        <canvas\r\n          ref={canvasRef}\r\n          style={{\r\n            position: \"absolute\",\r\n            marginLeft: \"auto\",\r\n            marginRight: \"auto\",\r\n            left: 0,\r\n            right: 0,\r\n            textAlign: \"center\",\r\n            zindex: 8,\r\n            width: 224,\r\n            height: 224,\r\n          }}\r\n        />\r\n      <p className=\"text-center\">{messgLoading}</p>\r\n    </div>\r\n  );\r\n}\r\n\r\nexport default App;\r\n","import React from 'react';\r\nimport ReactDOM from 'react-dom';\r\nimport './index.css';\r\nimport App from './App';\r\n\r\nReactDOM.render(\r\n  <React.StrictMode>\r\n    <div className=\"header\">\r\n      <h1 className=\"text-center\">Face mask detection</h1>\r\n    </div>\r\n    <App />\r\n  </React.StrictMode>,\r\n  document.getElementById('root')\r\n);"],"sourceRoot":""}